{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f7399-1966-4603-bc8f-537530fde58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "try:\n",
    "    from conformal_data_cleaning.config import error_fractions, error_types\n",
    "    from conformal_data_cleaning.data import _ID_TO_TASK_TYPE, AVAILABLE_DATASETS, TaskType\n",
    "    from conformal_data_cleaning.evaluation.utils import AutoInitializedDict\n",
    "    from conformal_data_cleaning.jenga_extension import OpenMLTask, get_OpenMLTask\n",
    "\n",
    "# package is not available in remote jupyter\n",
    "# for this reason, these files are copied over\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"..\")\n",
    "\n",
    "    from config import error_fractions, error_types\n",
    "    from evaluation.utils import AutoInitializedDict\n",
    "    from jenga_extension import OpenMLTask, get_OpenMLTask\n",
    "\n",
    "    from data import _ID_TO_TASK_TYPE, AVAILABLE_DATASETS, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbfe30d-c75b-448b-b0d1-6fc165c645be",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"final-experiments\"\n",
    "\n",
    "processed_path = Path(\"../../processed\")\n",
    "experiment_results_path = Path(f\"../../results/{experiment_name}\")\n",
    "experiment_models_path = Path(f\"../../models/{experiment_name}\")\n",
    "\n",
    "experiment_processed_path = processed_path / experiment_name\n",
    "if not experiment_processed_path.exists():\n",
    "    experiment_processed_path.mkdir()\n",
    "\n",
    "task_cache_file = processed_path / \"task_cache.joblib\"\n",
    "results_file = experiment_processed_path / \"results_cache.csv\"\n",
    "\n",
    "cleaned_data_cache_file = experiment_processed_path / \"cleaned_data_cache.joblib\"\n",
    "original_performances_cache_file = experiment_processed_path / \"original_performances_cache.joblib\"\n",
    "results_as_list_cache_file = experiment_processed_path / \"results_as_list_cache.joblib\"\n",
    "\n",
    "models_leaderboard_cache_file = processed_path / \"models_leaderboard_cache.csv\"\n",
    "dataset_statistics_file = processed_path / \"dataset_statistics.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efa910-7ecd-4d4a-aee2-33d73ce50529",
   "metadata": {},
   "source": [
    "# Calculate and Cache Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76126759-f464-466b-a060-d1d4f1dd8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(correct_task: OpenMLTask, to_test_data: pd.DataFrame, actual_error_fraction: float) -> tuple:\n",
    "    clean_test_data = correct_task.test_data\n",
    "\n",
    "    if actual_error_fraction == 0:\n",
    "        categorical_error = (1.0, 1.0) if len(correct_task.categorical_columns) > 0 else (None, None)\n",
    "        numerical_error = 0.0 if len(correct_task.numerical_columns) > 0 else None\n",
    "\n",
    "    else:\n",
    "        if len(correct_task.categorical_columns) > 0:\n",
    "            macro = f1_score(\n",
    "                clean_test_data[correct_task.categorical_columns].astype(str).to_numpy().flatten(),\n",
    "                to_test_data[correct_task.categorical_columns].astype(str).to_numpy().flatten(),\n",
    "                average=\"macro\",\n",
    "            )\n",
    "\n",
    "            weighted = f1_score(\n",
    "                clean_test_data[correct_task.categorical_columns].astype(str).to_numpy().flatten(),\n",
    "                to_test_data[correct_task.categorical_columns].astype(str).to_numpy().flatten(),\n",
    "                average=\"weighted\",\n",
    "            )\n",
    "\n",
    "            categorical_error = (macro, weighted)\n",
    "        else:\n",
    "            categorical_error = (None, None)\n",
    "\n",
    "        if len(correct_task.numerical_columns) > 0:\n",
    "            numerical_error = math.sqrt(\n",
    "                mean_squared_error(\n",
    "                    clean_test_data[correct_task.numerical_columns].to_numpy().flatten(),\n",
    "                    to_test_data[correct_task.numerical_columns].to_numpy().flatten(),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            numerical_error = None\n",
    "\n",
    "    return categorical_error, numerical_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877abab5-166b-4caf-a051-51c83fbcf6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check whether to use cache\n",
    "if not task_cache_file.exists() or not dataset_statistics_file.exists():\n",
    "    task_cache = AutoInitializedDict()\n",
    "    dataset_statistics_as_list = []\n",
    "\n",
    "    for i in trange(len(AVAILABLE_DATASETS)):\n",
    "        task_id = AVAILABLE_DATASETS[i]\n",
    "        correct_task = get_OpenMLTask(task_id=task_id)\n",
    "        task_cache[\"correct_task\"][task_id] = correct_task\n",
    "\n",
    "        clean_training_data = correct_task.train_data\n",
    "        clean_test_data = correct_task.test_data\n",
    "\n",
    "        task_type = _ID_TO_TASK_TYPE[task_id].value\n",
    "        num_categorical = len(correct_task.categorical_columns)\n",
    "        num_numerical = len(correct_task.numerical_columns)\n",
    "        num_training_examples = len(clean_training_data)\n",
    "\n",
    "        fraction_of_information_X_y = fraction_of_information(clean_training_data, correct_task.train_labels.to_frame())\n",
    "        fraction_of_information_column_to_rest = [\n",
    "            fraction_of_information(\n",
    "                clean_training_data[[x for x in clean_training_data.columns if x != column]],\n",
    "                clean_training_data[[column]],\n",
    "            )\n",
    "            for column in clean_training_data.columns\n",
    "        ]\n",
    "        fraction_of_information_column_to_rest_mean = np.mean(fraction_of_information_column_to_rest)\n",
    "        fraction_of_information_column_to_rest_std = np.std(fraction_of_information_column_to_rest)\n",
    "\n",
    "        for error_type in error_types:\n",
    "            for error_fraction in error_fractions:\n",
    "                corrupted_task = get_OpenMLTask(task_id=task_id, corruption=error_type, fraction=error_fraction)\n",
    "                corrupted_data = corrupted_task.test_data\n",
    "\n",
    "                error_mask = clean_test_data != corrupted_data\n",
    "                actual_error_fraction = error_mask.sum().sum() / corrupted_data.size\n",
    "                (categorical_error, categorical_error_weighted), numerical_error = calculate_errors(\n",
    "                    correct_task=correct_task,\n",
    "                    to_test_data=corrupted_data,\n",
    "                    actual_error_fraction=actual_error_fraction,\n",
    "                )\n",
    "\n",
    "                task_cache[\"corrupted_task\"][task_id][error_type][error_fraction][\"task\"] = corrupted_task\n",
    "                task_cache[\"corrupted_task\"][task_id][error_type][error_fraction][\"error_mask\"] = error_mask\n",
    "                dataset_statistics_as_list.append(\n",
    "                    (\n",
    "                        task_id,\n",
    "                        error_type,\n",
    "                        error_fraction,\n",
    "                        actual_error_fraction,\n",
    "                        task_type,\n",
    "                        num_categorical,\n",
    "                        num_numerical,\n",
    "                        num_training_examples,\n",
    "                        categorical_error,\n",
    "                        categorical_error_weighted,\n",
    "                        numerical_error,\n",
    "                        fraction_of_information_X_y,\n",
    "                        fraction_of_information_column_to_rest_mean,\n",
    "                        fraction_of_information_column_to_rest_std,\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "    dataset_statistics = (\n",
    "        pd.DataFrame(\n",
    "            dataset_statistics_as_list,\n",
    "            columns=[\n",
    "                \"task_id\",\n",
    "                \"error_type\",\n",
    "                \"error_fraction\",\n",
    "                \"actual_error_fraction\",\n",
    "                \"task_type\",\n",
    "                \"num_categorical\",\n",
    "                \"num_numerical\",\n",
    "                \"num_training_examples\",\n",
    "                \"categorical_error\",\n",
    "                \"categorical_error_weighted\",\n",
    "                \"numerical_error\",\n",
    "                \"fraction_of_information_X_y\",\n",
    "                \"fraction_of_information_column_to_rest_mean\",\n",
    "                \"fraction_of_information_column_to_rest_std\",\n",
    "            ],\n",
    "        )\n",
    "        # round 'actual_error_fraction' to 5 decimals\n",
    "        .assign(actual_error_fraction=lambda df: df[\"actual_error_fraction\"].apply(lambda x: round(x, 5)))\n",
    "        .convert_dtypes()\n",
    "    )\n",
    "\n",
    "    # save files\n",
    "    joblib.dump(task_cache, task_cache_file)\n",
    "    dataset_statistics.to_csv(dataset_statistics_file, index=False)\n",
    "\n",
    "else:\n",
    "    task_cache = joblib.load(task_cache_file)\n",
    "    dataset_statistics = pd.read_csv(dataset_statistics_file).convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44330236-12eb-4043-91f2-5411d4b8cf7f",
   "metadata": {},
   "source": [
    "# Cache Original Downstream Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92a1af-f44a-44fa-bb7d-9a279fb2ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether to use cache\n",
    "if not original_performances_cache_file.exists():\n",
    "    original_performances = AutoInitializedDict()\n",
    "\n",
    "    original_perf_paths = list(experiment_results_path.rglob(\"**/original_perf.json\"))\n",
    "    for i in trange(len(original_perf_paths)):\n",
    "        original_performance_file = original_perf_paths[i]\n",
    "\n",
    "        if not Path(*original_performance_file.parts[:7], \"finished.txt\").exists():\n",
    "            continue\n",
    "\n",
    "        task_id = int(original_performance_file.parts[4])\n",
    "        cleaner_type = str(original_performance_file.parts[5])\n",
    "        confidence_level = float(original_performance_file.parts[6])\n",
    "        repetition = int(original_performance_file.parts[7])\n",
    "        task_type = _ID_TO_TASK_TYPE[task_id]\n",
    "\n",
    "        if task_type == TaskType.BINARY or task_type == TaskType.MULTI_CLASS:\n",
    "            metric_name = \"F1_macro\"\n",
    "\n",
    "        elif task_type == TaskType.REGRESSION:\n",
    "            metric_name = \"RMSE\"\n",
    "\n",
    "        else:\n",
    "            raise Exception(f\"task_type ({task_type}) not valid.\")\n",
    "\n",
    "        original_performances[task_id][cleaner_type][confidence_level][repetition] = (\n",
    "            json.loads(original_performance_file.read_text())[metric_name],\n",
    "            metric_name,\n",
    "        )\n",
    "\n",
    "    # save files\n",
    "    joblib.dump(original_performances, original_performances_cache_file)\n",
    "\n",
    "else:\n",
    "    original_performances = joblib.load(original_performances_cache_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241714c8-741f-41f2-bd46-3bf6f9eafc41",
   "metadata": {},
   "source": [
    "# Cache Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e67af8-ff96-4bfd-989f-bec3f0e0b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether to use cache\n",
    "if not cleaned_data_cache_file.exists():\n",
    "    cleaned_data_cache = AutoInitializedDict()\n",
    "\n",
    "    cleaned_data_file_paths = list(experiment_results_path.rglob(\"**/cleaned_data.csv\"))\n",
    "    for i in trange(len(cleaned_data_file_paths)):\n",
    "        cleaned_data_file = cleaned_data_file_paths[i]\n",
    "        cleaned_mask_file = cleaned_data_file.parent / \"cleaned_mask.csv\"\n",
    "\n",
    "        if not Path(*cleaned_data_file.parts[:7], \"finished.txt\").exists():\n",
    "            continue\n",
    "\n",
    "        task_id = int(cleaned_data_file.parts[4])\n",
    "        cleaner_type = str(cleaned_data_file.parts[5])\n",
    "        confidence_level = float(cleaned_data_file.parts[6])\n",
    "        repetition = int(cleaned_data_file.parts[7])\n",
    "        error_type = cleaned_data_file.parts[8]\n",
    "        error_fraction = float(cleaned_data_file.parts[9])\n",
    "\n",
    "        cleaned_data = pd.read_csv(cleaned_data_file).convert_dtypes()\n",
    "        cleaned_mask = pd.read_csv(cleaned_mask_file).convert_dtypes()\n",
    "\n",
    "        cleaned_data_cache[task_id][cleaner_type][confidence_level][repetition][error_type][error_fraction][\n",
    "            \"data\"\n",
    "        ] = cleaned_data\n",
    "        cleaned_data_cache[task_id][cleaner_type][confidence_level][repetition][error_type][error_fraction][\n",
    "            \"mask\"\n",
    "        ] = cleaned_mask\n",
    "\n",
    "    # save files\n",
    "    joblib.dump(cleaned_data_cache, cleaned_data_cache_file)\n",
    "\n",
    "else:\n",
    "    cleaned_data_cache = joblib.load(cleaned_data_cache_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa121a84-3cd9-4623-bf09-4375fcf73eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read, Calculate, and Cache Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596138d-5789-4f9f-aa60-3d52381c7fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_downstream_performance_for(file: Path):\n",
    "    task_id = int(file.parts[4])\n",
    "    cleaner_type = str(file.parts[5])\n",
    "    confidence_level = float(file.parts[6])\n",
    "    repetition = int(file.parts[7])\n",
    "\n",
    "    original_performance, metric_name = original_performances[task_id][cleaner_type][confidence_level][repetition]\n",
    "    corrupted_performance_file = file.parent / \"corrupted_perf.json\"\n",
    "    cleaned_performance_file = file.parent / \"cleaned_perf.json\"\n",
    "\n",
    "    return (\n",
    "        original_performance,\n",
    "        json.loads(corrupted_performance_file.read_text())[metric_name],\n",
    "        json.loads(cleaned_performance_file.read_text())[metric_name],\n",
    "        metric_name,\n",
    "    )\n",
    "\n",
    "\n",
    "def read_and_calc_downstream_improvement_in_percent_for(file: Path):\n",
    "    (\n",
    "        original_performance,\n",
    "        corrupted_performance,\n",
    "        cleaned_performance,\n",
    "        metric_name,\n",
    "    ) = read_downstream_performance_for(file)\n",
    "\n",
    "    if metric_name == \"F1_macro\":\n",
    "        improvement_in_percent = (cleaned_performance - corrupted_performance) / corrupted_performance * 100\n",
    "\n",
    "    elif metric_name == \"RMSE\":\n",
    "        improvement_in_percent = -((cleaned_performance - corrupted_performance) / corrupted_performance) * 100\n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"metric_name ({metric_name}) not valid.\")\n",
    "\n",
    "    return (\n",
    "        original_performance,\n",
    "        corrupted_performance,\n",
    "        cleaned_performance,\n",
    "        metric_name,\n",
    "        improvement_in_percent,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataset_statistics_for(task_id: int, error_type: str, error_fraction: float) -> dict:\n",
    "    statistics = dataset_statistics[\n",
    "        (dataset_statistics[\"task_id\"] == task_id)\n",
    "        & (dataset_statistics[\"error_type\"] == error_type)\n",
    "        & (dataset_statistics[\"error_fraction\"] == error_fraction)\n",
    "    ]\n",
    "\n",
    "    if len(statistics) != 1:\n",
    "        raise Exception(\n",
    "            f\"Statistics for 'task_id={task_id}', 'error_type={error_type}', 'error_fraction={error_fraction}' have none or more than one entry\",\n",
    "        )\n",
    "\n",
    "    return statistics.iloc[0].to_dict()\n",
    "\n",
    "\n",
    "def is_improvement(row: pd.Series) -> bool:\n",
    "    if row[\"task_type\"] == \"regression\":\n",
    "        # RMSE\n",
    "        return row[\"cleaned_performance__mean\"] < row[\"corrupted_performance__mean\"]\n",
    "    else:\n",
    "        # F1\n",
    "        return row[\"cleaned_performance__mean\"] > row[\"corrupted_performance__mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55d81a-0d0c-4d5d-b70b-4514013d0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_as_list_cache_file.exists():\n",
    "    results_as_list = []\n",
    "\n",
    "    files = list(experiment_results_path.rglob(\"**/cleaned_perf.json\"))\n",
    "    for i in trange(len(files)):\n",
    "        file = files[i]\n",
    "\n",
    "        if not Path(*file.parts[:7], \"finished.txt\").exists():\n",
    "            continue\n",
    "\n",
    "        # get data from path\n",
    "        task_id = int(file.parts[4])\n",
    "        cleaner_type = str(file.parts[5])\n",
    "        confidence_level = float(file.parts[6])\n",
    "        repetition = int(file.parts[7])\n",
    "        error_type = file.parts[8]\n",
    "        error_fraction = float(file.parts[9])\n",
    "\n",
    "        # get and unpack intermediate/necessary data\n",
    "        try:\n",
    "            (\n",
    "                original_performance,\n",
    "                corrupted_performance,\n",
    "                cleaned_performance,\n",
    "                metric_name,\n",
    "                improvement_in_percent,\n",
    "            ) = read_and_calc_downstream_improvement_in_percent_for(file)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        current_dataset_statistics = get_dataset_statistics_for(\n",
    "            task_id=task_id,\n",
    "            error_type=error_type,\n",
    "            error_fraction=error_fraction,\n",
    "        )\n",
    "        actual_error_fraction = current_dataset_statistics[\"actual_error_fraction\"]\n",
    "        corrupted_categorical_error = current_dataset_statistics[\"categorical_error\"]\n",
    "        corrupted_categorical_weighted_error = current_dataset_statistics[\"categorical_error_weighted\"]\n",
    "        corrupted_numerical_error = current_dataset_statistics[\"numerical_error\"]\n",
    "        error_mask = task_cache[\"corrupted_task\"][task_id][error_type][error_fraction][\"error_mask\"]\n",
    "        cleaned_data = cleaned_data_cache[task_id][cleaner_type][confidence_level][repetition][error_type][\n",
    "            error_fraction\n",
    "        ][\"data\"]\n",
    "        cleaned_mask = cleaned_data_cache[task_id][cleaner_type][confidence_level][repetition][error_type][\n",
    "            error_fraction\n",
    "        ][\"mask\"]\n",
    "        correct_data = task_cache[\"correct_task\"][task_id].test_data\n",
    "\n",
    "        if cleaner_type == \"ConformalAutoGluon\":\n",
    "            prediction_sets_dict = pickle.loads((file.parent / \"prediction_sets.pckl\").read_bytes())\n",
    "            coverages = []\n",
    "            empty_set_fractions = []\n",
    "            average_set_sizes = []\n",
    "            relative_average_set_sizes = []\n",
    "\n",
    "            for column_name, prediction_sets in prediction_sets_dict.items():\n",
    "                if column_name in task_cache[\"correct_task\"][task_id].categorical_columns:\n",
    "                    cardinality = len(task_cache[\"correct_task\"][task_id].train_data[column_name].unique())\n",
    "                    true_value_in_prediction_set = np.any(\n",
    "                        prediction_sets == correct_data[column_name].to_numpy()[:, np.newaxis],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    coverages.append(true_value_in_prediction_set.mean())\n",
    "\n",
    "                    average_set_sizes.append((~pd.DataFrame(prediction_sets).isna()).sum(axis=1).mean())\n",
    "                    relative_average_set_sizes.append(average_set_sizes[-1] / cardinality)\n",
    "                    empty_set_fractions.append(((~pd.DataFrame(prediction_sets).isna()).sum(axis=1) == 0).mean())\n",
    "\n",
    "                elif column_name in task_cache[\"correct_task\"][task_id].numerical_columns:\n",
    "                    value_range = (\n",
    "                        task_cache[\"correct_task\"][task_id].train_data[column_name].max()\n",
    "                        - task_cache[\"correct_task\"][task_id].train_data[column_name].min()\n",
    "                    )\n",
    "                    true_value_in_prediction_range = (correct_data[column_name].to_numpy() >= prediction_sets[:, 0]) & (\n",
    "                        correct_data[column_name].to_numpy() <= prediction_sets[:, 2]\n",
    "                    )\n",
    "                    coverages.append(true_value_in_prediction_range.mean())\n",
    "                    average_set_sizes.append((prediction_sets[:, 2] - prediction_sets[:, 0]).mean())\n",
    "                    relative_average_set_sizes.append(average_set_sizes[-1] / value_range)\n",
    "                    empty_set_fractions.append(((prediction_sets[:, 2] - prediction_sets[:, 0]) == 0).mean())\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\n",
    "                        f\"Type (categorical or numerical) of '{column_name}' of task '{task_id}' is not known!\",\n",
    "                    )\n",
    "\n",
    "            coverage = np.mean(coverages)\n",
    "            empty_set_fraction = np.mean(empty_set_fractions)\n",
    "            average_set_size = np.mean(average_set_sizes)\n",
    "            relative_average_set_size = np.mean(relative_average_set_sizes)\n",
    "\n",
    "        else:\n",
    "            coverage = empty_set_fraction = average_set_size = relative_average_set_size = None\n",
    "\n",
    "        # calculate how well errors are detected/fixed\n",
    "        number_of_errors = error_mask.sum().sum()\n",
    "        number_of_assumed_errors = cleaned_mask.sum().sum()\n",
    "        error_detection_fraction = (\n",
    "            (error_mask & cleaned_mask).sum().sum() / number_of_errors if number_of_errors > 0 else None\n",
    "        )  # relative to number of errors\n",
    "        error_wrong_detection_fraction = (~error_mask & cleaned_mask).sum().sum() / (\n",
    "            ~error_mask\n",
    "        ).sum().sum()  # relative to number of correct cells\n",
    "        error_fraction_after_cleaning = (correct_data != cleaned_data).sum().sum() / correct_data.size\n",
    "        error_reduction = actual_error_fraction - error_fraction_after_cleaning\n",
    "\n",
    "        (cleaned_categorical_error, cleaned_categorical_error_weighted), cleaned_numerical_error = calculate_errors(\n",
    "            correct_task=task_cache[\"correct_task\"][task_id],\n",
    "            to_test_data=cleaned_data,\n",
    "            actual_error_fraction=actual_error_fraction,\n",
    "        )\n",
    "\n",
    "        results_as_list.append(\n",
    "            (\n",
    "                task_id,\n",
    "                cleaner_type,\n",
    "                confidence_level,\n",
    "                error_type,\n",
    "                error_fraction,\n",
    "                actual_error_fraction,\n",
    "                repetition,\n",
    "                original_performance,\n",
    "                corrupted_performance,\n",
    "                cleaned_performance,\n",
    "                improvement_in_percent,\n",
    "                cleaned_categorical_error,\n",
    "                cleaned_categorical_error_weighted,\n",
    "                cleaned_numerical_error,\n",
    "                cleaned_categorical_error - corrupted_categorical_error\n",
    "                if cleaned_categorical_error is not None and ~pd.isna(corrupted_categorical_error)\n",
    "                else None,\n",
    "                cleaned_categorical_error_weighted - corrupted_categorical_weighted_error\n",
    "                if cleaned_categorical_error_weighted is not None and ~pd.isna(corrupted_categorical_weighted_error)\n",
    "                else None,\n",
    "                corrupted_numerical_error - cleaned_numerical_error\n",
    "                if cleaned_numerical_error is not None and ~pd.isna(corrupted_numerical_error)\n",
    "                else None,\n",
    "                error_detection_fraction,\n",
    "                error_wrong_detection_fraction,\n",
    "                error_fraction_after_cleaning,\n",
    "                error_reduction,\n",
    "                coverage,\n",
    "                empty_set_fraction,\n",
    "                average_set_size,\n",
    "                relative_average_set_size,\n",
    "                number_of_errors,\n",
    "                number_of_assumed_errors,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    joblib.dump(results_as_list, results_as_list_cache_file)\n",
    "\n",
    "else:\n",
    "    results_as_list = joblib.load(results_as_list_cache_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41fa67-bbc4-4c29-8dd0-b2b6a7a05c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_statistics = pd.read_csv(dataset_statistics_file).convert_dtypes()\n",
    "\n",
    "if not results_file.exists():\n",
    "    results = (\n",
    "        pd.DataFrame(\n",
    "            results_as_list,\n",
    "            columns=[\n",
    "                \"task_id\",\n",
    "                \"cleaner_type\",\n",
    "                \"confidence_level\",\n",
    "                \"error_type\",\n",
    "                \"error_fraction\",\n",
    "                \"actual_error_fraction\",\n",
    "                \"repetition\",\n",
    "                \"original_performance\",\n",
    "                \"corrupted_performance\",\n",
    "                \"cleaned_performance\",\n",
    "                \"improvement_in_percent\",\n",
    "                \"cleaned_categorical_error\",\n",
    "                \"cleaned_categorical_error_weighted\",\n",
    "                \"cleaned_numerical_error\",\n",
    "                \"categorical_error_reduction\",\n",
    "                \"categorical_error_weighted_reduction\",\n",
    "                \"numerical_error_reduction\",\n",
    "                \"error_detection_fraction\",\n",
    "                \"error_wrong_detection_fraction\",\n",
    "                \"error_fraction_after_cleaning\",\n",
    "                \"error_reduction\",\n",
    "                \"coverage\",\n",
    "                \"empty_set_fraction\",\n",
    "                \"average_set_size\",\n",
    "                \"relative_average_set_size\",\n",
    "                \"number_of_errors\",\n",
    "                \"number_of_assumed_errors\",\n",
    "            ],\n",
    "        )\n",
    "        .convert_dtypes()\n",
    "        # calculate mean/std over 3 repetitions\n",
    "        .groupby(\n",
    "            [\n",
    "                \"task_id\",\n",
    "                \"cleaner_type\",\n",
    "                \"confidence_level\",\n",
    "                \"error_type\",\n",
    "                \"error_fraction\",\n",
    "                \"actual_error_fraction\",\n",
    "            ],\n",
    "        )\n",
    "        .agg([\"mean\", \"std\"])\n",
    "        # remove no longer necessary repetition column\n",
    "        .drop(columns=\"repetition\", level=0)\n",
    "        .reset_index()\n",
    "        # fix column names\n",
    "        .pipe(lambda df: df.set_axis(df.columns.map(lambda x: f\"{x[0]}__{x[1]}\" if x[1] else x[0]), axis=1))\n",
    "        # round 'actual_error_fraction' to 5 decimals\n",
    "        .assign(actual_error_fraction=lambda df: df[\"actual_error_fraction\"].apply(lambda x: round(x, 5)))\n",
    "        .merge(\n",
    "            dataset_statistics,\n",
    "            on=[\"task_id\", \"error_type\", \"error_fraction\", \"actual_error_fraction\"],\n",
    "            how=\"left\",\n",
    "            validate=\"m:1\",\n",
    "        )\n",
    "        .assign(has_errors=lambda df: df[\"actual_error_fraction\"] > 0)\n",
    "        .assign(improvement=lambda df: df.apply(is_improvement, axis=1))\n",
    "    )\n",
    "\n",
    "    results.to_csv(results_file, index=False)\n",
    "\n",
    "else:\n",
    "    results = pd.read_csv(results_file).convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1d98c3-7e48-4894-9c50-5435b29953bd",
   "metadata": {},
   "source": [
    "# Read and Cache Model Leaderboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c2338-e5f2-4f42-b455-05bac453a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_statistics = pd.read_csv(dataset_statistics_file).convert_dtypes()\n",
    "\n",
    "if not models_leaderboard_cache_file.exists():\n",
    "    models_leaderboard_as_list = []\n",
    "\n",
    "    models_leaderboard_file_paths = list(experiment_models_path.rglob(\"**/*.csv\"))\n",
    "    for index in trange(len(models_leaderboard_file_paths)):\n",
    "        leaderboard_temp = pd.read_csv(models_leaderboard_file_paths[index])\n",
    "        leaderboard_temp[\"task_id\"] = int(models_leaderboard_file_paths[index].parts[4])\n",
    "        leaderboard_temp[\"cleaner_type\"] = models_leaderboard_file_paths[index].parts[5]\n",
    "        leaderboard_temp[\"confidence_level\"] = float(models_leaderboard_file_paths[index].parts[6])\n",
    "        leaderboard_temp[\"repetition\"] = int(models_leaderboard_file_paths[index].parts[7])\n",
    "\n",
    "        models_leaderboard_as_list.append(leaderboard_temp)\n",
    "\n",
    "    # create single dataframe\n",
    "    models_leaderboard = pd.concat(models_leaderboard_as_list, ignore_index=True)\n",
    "\n",
    "    (\n",
    "        # drop some not necessary columns\n",
    "        models_leaderboard.drop(\n",
    "            columns=[\n",
    "                \"ag_args_fit\",\n",
    "                \"ancestors\",\n",
    "                \"can_infer\",\n",
    "                \"child_ag_args_fit\",\n",
    "                \"child_hyperparameters\",\n",
    "                \"child_hyperparameters_fit\",\n",
    "                \"child_model_type\",\n",
    "                \"descendants\",\n",
    "                \"features\",\n",
    "                \"fit_order\",\n",
    "                \"fit_time_marginal\",\n",
    "                \"memory_size\",\n",
    "                \"memory_size_min\",\n",
    "                \"memory_size_min_w_ancestors\",\n",
    "                \"memory_size_w_ancestors\",\n",
    "                \"num_ancestors\",\n",
    "                \"num_descendants\",\n",
    "                \"num_features\",\n",
    "                \"num_models\",\n",
    "                \"num_models_w_ancestors\",\n",
    "                \"pred_time_val\",\n",
    "                \"pred_time_val_marginal\",\n",
    "                \"score_val\",\n",
    "                \"stack_level\",\n",
    "            ],\n",
    "        )\n",
    "        # add 'task_type' column\n",
    "        .merge(\n",
    "            dataset_statistics[[\"task_id\", \"task_type\"]].drop_duplicates(),\n",
    "            on=\"task_id\",\n",
    "            how=\"inner\",\n",
    "            validate=\"m:m\",\n",
    "        )\n",
    "        # finally, save as csv\n",
    "        .to_csv(models_leaderboard_cache_file, index=False)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    models_leaderboard = pd.read_csv(models_leaderboard_cache_file).convert_dtypes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1e079ba1052bcc85be9b78b3c72bf03a4d015549529e67f43de37f743ce1597"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
